#!/usr/bin/env python3
"""
Test Script for Phase 1 Fixes

Tests the implementation of:
1. FFmpeg/Audio extraction robustness
2. Captions (SRT) presence + fallbacks
3. Legibility defaults + WCAG-AA contrast gate
4. Duration policy enforcement
5. Determinism
"""

import os
import sys
import tempfile

# Ensure repo root on path
ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)

from bin.core import get_logger

log = get_logger("phase1_test")


def test_ffmpeg_validation():
    """Test FFmpeg validation functionality"""
    print("=== Testing FFmpeg Validation ===")

    try:
        from bin.audio_validator import FFmpegValidator

        # Test FFmpeg validator initialization
        validator = FFmpegValidator()
        print("‚úÖ FFmpegValidator initialized successfully")

        # Test audio stream probing (use a test file if available)
        test_video = os.path.join(ROOT, "videos", "test_video.mp4")
        if os.path.exists(test_video):
            try:
                stream_info = validator.probe_audio_stream(test_video)
                print(
                    f"‚úÖ Audio stream probed successfully: {stream_info.get('codec_name', 'unknown')}"
                )
            except Exception as e:
                print(f"‚ö†Ô∏è Audio stream probing failed: {e}")
        else:
            print("‚ö†Ô∏è Test video not found, skipping audio stream test")

        return True

    except Exception as e:
        print(f"‚ùå FFmpeg validation test failed: {e}")
        return False


def test_srt_generation():
    """Test SRT generation functionality"""
    print("\n=== Testing SRT Generation ===")

    try:
        from bin.srt_generate import generate_srt_for_acceptance

        # Test with a sample script
        test_script = os.path.join(ROOT, "scripts", "test_script.txt")
        if os.path.exists(test_script):
            # Create temporary output
            with tempfile.NamedTemporaryFile(suffix=".srt", delete=False) as tmp:
                output_path = tmp.name

            try:
                result = generate_srt_for_acceptance(
                    script_path=test_script,
                    output_path=output_path,
                    intent_type="narrative_history",
                )

                if result["success"]:
                    print(
                        f"‚úÖ SRT generated successfully: {result['generation_method']}"
                    )
                    print(f"   Duration: {result['duration_sec']:.2f}s")
                    print(f"   Word count: {result['word_count']}")

                    # Clean up
                    os.unlink(output_path)
                    return True
                else:
                    print(f"‚ùå SRT generation failed: {result['error']}")
                    return False

            except Exception as e:
                print(f"‚ùå SRT generation test error: {e}")
                return False
        else:
            print("‚ö†Ô∏è Test script not found, skipping SRT generation test")
            return True

    except Exception as e:
        print(f"‚ùå SRT generation test failed: {e}")
        return False


def test_legibility_validation():
    """Test legibility validation functionality"""
    print("\n=== Testing Legibility Validation ===")

    try:
        from bin.legibility import (
            inject_safe_background,
            validate_contrast_for_acceptance,
        )

        # Test contrast validation
        test_cases = [
            ("#000000", "#FFFFFF", "black_on_white"),  # High contrast
            ("#000000", "#808080", "black_on_gray"),  # Medium contrast
            ("#000000", "#000000", "black_on_black"),  # No contrast
        ]

        for text_color, bg_color, test_name in test_cases:
            result = validate_contrast_for_acceptance(text_color, bg_color, test_name)
            status = "‚úÖ" if result["valid"] else "‚ùå"
            print(
                f"{status} {test_name}: contrast {result.get('contrast_ratio', 'N/A')}"
            )

        # Test background injection
        result = inject_safe_background("#000000", "test_element")
        if result["success"]:
            print(
                f"‚úÖ Background injection successful: {result['injected_background']}"
            )
        else:
            print(f"‚ùå Background injection failed: {result['error']}")

        return True

    except Exception as e:
        print(f"‚ùå Legibility validation test failed: {e}")
        return False


def test_duration_policy():
    """Test duration policy enforcement"""
    print("\n=== Testing Duration Policy ===")

    try:
        # Test with sample SceneScript data
        test_scenescript = {
            "scenes": [
                {"duration_ms": 5000},  # 5s
                {"duration_ms": 8000},  # 8s
                {"duration_ms": 12000},  # 12s
            ]
        }

        # This would normally be tested through the acceptance pipeline
        # For now, just verify the configuration is loaded
        render_config_path = os.path.join(ROOT, "conf", "render.yaml")
        if os.path.exists(render_config_path):
            print("‚úÖ render.yaml found with duration policy settings")

            # Check for key settings
            with open(render_config_path, "r") as f:
                import yaml

                config = yaml.safe_load(f)

            acceptance = config.get("acceptance", {})
            tolerance = acceptance.get("tolerance_pct", 0)
            min_scene = acceptance.get("min_scene_ms", 0)
            max_scene = acceptance.get("max_scene_ms", 0)

            print(f"   Duration tolerance: ¬±{tolerance}%")
            print(f"   Scene bounds: {min_scene}-{max_scene}ms")

            return True
        else:
            print("‚ùå render.yaml not found")
            return False

    except Exception as e:
        print(f"‚ùå Duration policy test failed: {e}")
        return False


def test_acceptance_integration():
    """Test acceptance pipeline integration"""
    print("\n=== Testing Acceptance Integration ===")

    try:
        # Test acceptance validator initialization
        from bin.acceptance import AcceptanceValidator

        # This would require a full config setup, so just test import
        print("‚úÖ AcceptanceValidator imported successfully")

        # Check if new methods are available
        validator_class = AcceptanceValidator
        required_methods = [
            "_validate_captions",
            "_validate_legibility",
            "_validate_determinism",
            "_check_srt_consistency",
        ]

        for method in required_methods:
            if hasattr(validator_class, method):
                print(f"‚úÖ Method {method} available")
            else:
                print(f"‚ùå Method {method} missing")
                return False

        return True

    except Exception as e:
        print(f"‚ùå Acceptance integration test failed: {e}")
        return False


def test_configuration():
    """Test configuration files"""
    print("\n=== Testing Configuration ===")

    try:
        # Check render.yaml
        render_path = os.path.join(ROOT, "conf", "render.yaml")
        if os.path.exists(render_path):
            print("‚úÖ render.yaml exists")

            with open(render_path, "r") as f:
                import yaml

                config = yaml.safe_load(f)

            # Check required sections
            required_sections = ["acceptance", "audio", "captions"]
            for section in required_sections:
                if section in config:
                    print(f"   ‚úÖ {section} section present")
                else:
                    print(f"   ‚ùå {section} section missing")
                    return False

            # Check acceptance settings
            acceptance = config.get("acceptance", {})
            required_settings = [
                "tolerance_pct",
                "audio_validation_required",
                "caption_validation_required",
            ]
            for setting in required_settings:
                if setting in acceptance:
                    print(f"   ‚úÖ acceptance.{setting} configured")
                else:
                    print(f"   ‚ùå acceptance.{setting} missing")
                    return False

            return True
        else:
            print("‚ùå render.yaml not found")
            return False

    except Exception as e:
        print(f"‚ùå Configuration test failed: {e}")
        return False


def main():
    """Run all Phase 1 fix tests"""
    print("üöÄ Phase 1 Fixes Test Suite")
    print("=" * 50)

    tests = [
        ("FFmpeg Validation", test_ffmpeg_validation),
        ("SRT Generation", test_srt_generation),
        ("Legibility Validation", test_legibility_validation),
        ("Duration Policy", test_duration_policy),
        ("Acceptance Integration", test_acceptance_integration),
        ("Configuration", test_configuration),
    ]

    results = []
    for test_name, test_func in tests:
        try:
            success = test_func()
            results.append((test_name, success))
        except Exception as e:
            print(f"‚ùå {test_name} test crashed: {e}")
            results.append((test_name, False))

    # Summary
    print("\n" + "=" * 50)
    print("üìä Test Results Summary")
    print("=" * 50)

    passed = 0
    total = len(results)

    for test_name, success in results:
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        print(f"{status} {test_name}")
        if success:
            passed += 1

    print(f"\nOverall: {passed}/{total} tests passed")

    if passed == total:
        print("üéâ All Phase 1 fixes are working correctly!")
        return 0
    else:
        print("‚ö†Ô∏è Some Phase 1 fixes need attention")
        return 1


if __name__ == "__main__":
    sys.exit(main())
