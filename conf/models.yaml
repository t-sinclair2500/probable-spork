# Model Configuration for Single-Model Pipeline
# Everything uses llama3.2:3b for simplicity and compatibility

profile: "laptop_m2_8gb"  # Hardware profile for optimization hints

ollama:
  base_url: "http://127.0.0.1:11434"
  timeout_sec: 60

defaults:
  chat_model: "llama3.2:3b"
  generate_model: "llama3.2:3b"
  embeddings_model: "nomic-embed-text"

options:
  # Sent to Ollama in the "options" payload for chat/generate
  num_ctx: 4096
  num_predict: 512
  temperature: 0.4
  seed: 1337
  # Remove hardcoded CPU-only; allow GPU where applicable (Ollama manages device on macOS via Metal)
  # Set num_gpu to -1 to offload as much as possible; leave unset to let Ollama decide
  # num_gpu: -1

models:
  # All tasks use the same model - llama3.2:3b
  research:
    name: "llama3.2:3b"
    num_ctx: 4096
    temperature: 0.3
    top_p: 0.9
    repeat_penalty: 1.1
    description: "Used for research planning, fact-guard, and reasoning tasks"
  
  # Scriptwriting - same model, different temperature
  scriptwriter:
    name: "llama3.2:3b"
    num_ctx: 8192
    temperature: 0.7
    top_p: 0.9
    repeat_penalty: 1.1
    description: "Used for script generation and creative writing"
  
  # Outlining - same model, structured planning
  outline:
    name: "llama3.2:3b"
    num_ctx: 4096
    temperature: 0.3
    top_p: 0.9
    repeat_penalty: 1.1
    description: "Used for outline generation and structure planning"
  
  # Clustering - same model, topic organization
  cluster:
    name: "llama3.2:3b"
    num_ctx: 4096
    temperature: 0.2
    top_p: 0.9
    repeat_penalty: 1.1
    description: "Used for topic clustering and categorization"

voice:
  # Text-to-Speech with Piper
  tts:
    provider: "piper"
    voice_id: "en_US-amy-medium"
    rate: "medium"  # slow, medium, fast
    pitch: 0  # -20 to +20
    pause_ms: 120  # Default pause between sentences
    ssml: true  # Enable SSML-lite parsing
    lufs_target: -16.0  # Loudness normalization target
    cache_dir: "voice_cache"  # Directory for audio caching
  
  # Speech-to-Text with Whisper
  asr:
    provider: "whisper.cpp"
    model: "small.en"
    language: "en"
    task: "transcribe"

# Model identifiers for unified access
script_model: "llama3.2:3b"
outline_model: "llama3.2:3b"
caption_asr_model: "whisper-small-int8"
tts_voice: "piper/en_US-amy-medium"
